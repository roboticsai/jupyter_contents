{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 5)\n",
      "(97, 5)\n",
      "    Position  Year         Make                           Model     Time\n",
      "0          1  2009      Radical                          SR8 LM  06:48.0\n",
      "1          2  2005      Radical                             SR8  06:56.1\n",
      "2          3  2013      Porsche                      918 Spyder  06:57.0\n",
      "3          4  2015  Lamborghini  Aventador LP 750-4 Superveloce  06:59.7\n",
      "4          5  2015       Nissan                      GT-R Nismo  07:08.7\n",
      "..       ...   ...          ...                             ...      ...\n",
      "95        96  2003      Ferrari                          360 CS  07:56.0\n",
      "96        97  2009      Ferrari                   California GT  07:56.0\n",
      "97        98  2009      Porsche     Panamera Sport Chrono Turbo  07:56.0\n",
      "98        99  2009      Porsche                  Panamera Turbo  07:56.0\n",
      "99       100  2002    Chevrolet                    Corvette Z06  07:56.0\n",
      "\n",
      "[97 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "'''16. Write a python program to remove all entries with a\n",
    "null value present.'''\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"lap_times.csv\")\n",
    "pd.isnull(df).any()\n",
    "print(df.shape)\n",
    "df = df.dropna(how='any',axis=0)\n",
    "pd.isnull(df).any()\n",
    "print(df.shape)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "    one  two\n",
      "a  1.0  1.0\n",
      "b  2.0  2.0\n",
      "c  NaN  3.0\n",
      "d  NaN  4.0\n",
      "e  5.0  NaN\n",
      "\n",
      "After:\n",
      "      one    two\n",
      "a    1.0    1.0\n",
      "b    2.0    2.0\n",
      "c  139.0    3.0\n",
      "d  139.0    4.0\n",
      "e    5.0  139.0\n"
     ]
    }
   ],
   "source": [
    "'''17. Write a python program to replace all missing values in the dataset with a desired value accepted\n",
    "from the user.'''\n",
    "import pandas\n",
    "import numpy as np\n",
    "data = {'one': pandas.Series([1,2,5], index=['a','b','e']),\n",
    "       'two': pandas.Series([1,2,3,4], index=['a','b','c','d'])\n",
    "       }\n",
    "table = pandas.DataFrame(data)\n",
    "print(\"Before:\\n\",table)\n",
    "print()\n",
    "buff = 139\n",
    "table['one'] = table['one'].replace(np.nan, buff)\n",
    "table['two'].fillna(buff, inplace = True)\n",
    "print(\"After:\\n\",table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "    one  two\n",
      "a  1.0  1.0\n",
      "b  2.0  2.0\n",
      "c  NaN  3.0\n",
      "d  NaN  4.0\n",
      "e  5.0  NaN\n",
      "\n",
      "After:\n",
      "         One  Two\n",
      "0  1.000000  1.0\n",
      "1  2.000000  2.0\n",
      "2  2.666667  3.0\n",
      "3  2.666667  4.0\n",
      "4  5.000000  2.5\n"
     ]
    }
   ],
   "source": [
    "'''18. Develop a Python code to impute a mean value to all\n",
    "the null fields present in the data provided.'''\n",
    "import pandas\n",
    "import numpy as np\n",
    "data = {'one':pandas.Series([1,2,5], index=['a','b','e']),\n",
    "        'two':pandas.Series([1,2,3,4], index=['a','b','c','d'])\n",
    "       }\n",
    "table = pandas.DataFrame(data)\n",
    "print(\"Before:\\n\",table)\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer.fit(table)\n",
    "imputed_data = imputer.transform(table.values)\n",
    "print(\"\\nAfter:\\n\", pandas.DataFrame(imputed_data, columns=['One','Two']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Customer_id       Loan_type Income\n",
      "0            1       Home Loan    30K\n",
      "1            2   Personal Loan    25K\n",
      "2            3  Education Loan    15K\n",
      "3            4       Home Loan    40K\n",
      "4            5     Credit Loan    35K\n",
      "\n",
      "   Customer_id  Loan_type_Education Loan  Loan_type_Home Loan  \\\n",
      "0            1                         0                    1   \n",
      "1            2                         0                    0   \n",
      "2            3                         1                    0   \n",
      "3            4                         0                    1   \n",
      "4            5                         0                    0   \n",
      "\n",
      "   Loan_type_Personal Loan  Income_25K  Income_30K  Income_35K  Income_40K  \n",
      "0                        0           0           1           0           0  \n",
      "1                        1           1           0           0           0  \n",
      "2                        0           0           0           0           0  \n",
      "3                        0           0           0           0           1  \n",
      "4                        0           0           0           1           0  \n"
     ]
    }
   ],
   "source": [
    "'''19. Write a pythonic method to convert categorical data\n",
    "into numerical data.'''\n",
    "import pandas as pd\n",
    "data = {\n",
    "    'Customer_id': pd.Series([1,2,3,4,5]),\n",
    "    'Loan_type': pd.Series(['Home Loan','Personal Loan','Education Loan',\n",
    "                           'Home Loan','Credit Loan']),\n",
    "    'Income': pd.Series(['30K','25K','15K','40K','35K'])\n",
    "}\n",
    "loan_info = pd.DataFrame(data)\n",
    "print(loan_info)\n",
    "print()\n",
    "loan_info = pd.get_dummies(loan_info, prefix_sep='_', drop_first=True)\n",
    "print(loan_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Welcome to Python.org</title>\n"
     ]
    }
   ],
   "source": [
    "'''1. Write a Python program to fetch the title of the URL\n",
    "specified.'''\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "url = 'https://www.python.org/'\n",
    "req = requests.session()\n",
    "content = req.get(url)\n",
    "soup = BeautifulSoup(content.text, 'html.parser')\n",
    "#print(soup.prettify())\n",
    "print(soup.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Content of elements that contain 'Python'string:\n",
      "Welcome to Python.org\n",
      "Python\n",
      "The Python Network\n",
      "Python Brochure\n",
      "Python Books\n",
      "Python Essays\n",
      "Python Conferences\n",
      "Python Logo\n",
      "Python Wiki\n",
      "Python News\n",
      "Python Events\n",
      "Python Events Archive\n",
      "# Python 3: Fibonacci series up to n\n",
      "The core of extensible programming is defining functions. Python allows mandatory and optional arguments, keyword arguments, and even arbitrary argument lists.\n",
      "More about defining functions in Python 3\n",
      "# Python 3: List comprehensions\n",
      "Lists (known as arrays in other languages) are one of the compound data types that Python understands. Lists can be indexed, sliced and manipulated with other built-in functions.\n",
      "More about lists in Python 3\n",
      "# Python 3: Simple arithmetic\n",
      "Calculations are simple with Python, and expression syntax is straightforward: the operators\n",
      "More about simple math functions in Python 3\n",
      "# Python 3: Simple output (with Unicode)\n",
      ">>> print(\"Hello, I'm Python!\")\n",
      "Hello, I'm Python!\n",
      "What is your name? Python Hi, Python.\n",
      "Experienced programmers in any other language can pick up Python very quickly, and beginners find the clean syntax and indentation structure easy to learn.\n",
      "with our Python 3 overview.\n",
      "Python knows the usual control flow statements that other languages speak —\n",
      "More control flow tools in Python 3\n",
      "Python is a programming language that lets you work quickly\n",
      "Whether you're new to programming or an experienced developer, it's easy to learn and use Python.\n",
      "Python source code and installers are available for download for all versions!\n",
      "Python 3.8.5\n",
      "Documentation for Python's standard library, along with tutorials and guides, are available online.\n",
      "Looking for work or have a Python related position that you're trying to hire for? Our\n",
      "Python Software Foundation - July 2020 Newsletter\n",
      "Python 3.8.4 is now available\n",
      "Python 3.9.0b4 is now ready for testing\n",
      "EuroPython 2020 Online\n",
      "Python's convenience has made it the most popular language for machine learning and artificial intelligence. Python's flexibility has allowed Anyscale to make ML/AI scalable from laptops to clusters.\n",
      "Python provides convenience and flexibility for scalable ML/AI\n",
      "Use Python for…\n",
      "wxPython\n",
      "IPython\n",
      "Python Enhancement Proposals\n",
      ": The future of Python\n",
      "Python Software Foundation\n",
      "The mission of the Python Software Foundation is to promote, protect, and advance the Python programming language, and to support and facilitate the growth of a diverse and international community of Python programmers.\n",
      "Python Brochure\n",
      "Python Books\n",
      "Python Essays\n",
      "Python Conferences\n",
      "Python Logo\n",
      "Python Wiki\n",
      "Python News\n",
      "Python Events\n",
      "Python Events Archive\n",
      "Python Software Foundation\n"
     ]
    }
   ],
   "source": [
    "'''2. Write a Python program to print content of elements\n",
    "that contain a specified string of a given web page.'''\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup as bs\n",
    "url = 'https://www.python.org/'\n",
    "reqs = requests.get(url)\n",
    "soup = bs(reqs.text, 'html.parser')\n",
    "print(\"\\n Content of elements that contain 'Python'string:\")\n",
    "str1 = soup.find_all(string=re.compile('Python'))\n",
    "for txt in str1:\n",
    "    print(\" \".join(txt.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter URL of the image required:https://docs.python.org/3/library/re.html\n",
      "Received a chunk\n"
     ]
    }
   ],
   "source": [
    "'''3. Write a Python program to download an image from\n",
    "the URL provided.'''\n",
    "import requests\n",
    "url = input('Enter URL of the image required:')\n",
    "req = requests.get(url, stream=True)\n",
    "req.raise_for_status()\n",
    "with open('REQDownload.jpg', 'wb') as fd:\n",
    "    for chunk in req.iter_content(chunk_size=50000):\n",
    "        print('Received a chunk')\n",
    "        fd.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the URL to scrape:https://docs.python.org/3/library/re.html\n",
      "../genindex.html\n",
      "../py-modindex.html\n",
      "difflib.html\n",
      "string.html\n",
      "https://www.python.org/\n",
      "../index.html\n",
      "index.html\n",
      "text.html\n",
      "#module-re\n",
      "#module-re\n",
      "https://github.com/python/cpython/tree/3.8/Lib/re.py\n",
      "stdtypes.html#str\n",
      "stdtypes.html#bytes\n",
      "exceptions.html#DeprecationWarning\n",
      "exceptions.html#SyntaxError\n",
      "#re-objects\n",
      "https://pypi.org/project/regex/\n",
      "#module-re\n",
      "#regular-expression-syntax\n",
      "#frie09\n",
      "../howto/regex.html#regex-howto\n",
      "#re.DOTALL\n",
      "#re.MULTILINE\n",
      "#re.MULTILINE\n",
      "#re.MULTILINE\n",
      "#re.ASCII\n",
      "#re.LOCALE\n",
      "https://unicode.org/reports/tr18/\n",
      "exceptions.html#FutureWarning\n",
      "exceptions.html#FutureWarning\n",
      "#re.A\n",
      "#re.I\n",
      "#re.L\n",
      "#re.M\n",
      "#re.S\n",
      "#re.X\n",
      "#contents-of-module-re\n",
      "#re.compile\n",
      "#re.A\n",
      "#re.I\n",
      "#re.L\n",
      "#re.M\n",
      "#re.S\n",
      "#re.X\n",
      "#contents-of-module-re\n",
      "#re.search\n",
      "#re.match\n",
      "#re.ASCII\n",
      "#re.LOCALE\n",
      "#re.ASCII\n",
      "#re.LOCALE\n",
      "#re.ASCII\n",
      "#re.ASCII\n",
      "#re.ASCII\n",
      "#re.ASCII\n",
      "#re.ASCII\n",
      "#re.LOCALE\n",
      "#re.ASCII\n",
      "#re.LOCALE\n",
      "#module-contents\n",
      "enum.html#enum.IntFlag\n",
      "#re.compile\n",
      "#re-objects\n",
      "#re.Pattern.match\n",
      "#re.Pattern.search\n",
      "#re.compile\n",
      "#re.compile\n",
      "#re.A\n",
      "#re.ASCII\n",
      "#re.DEBUG\n",
      "#re.I\n",
      "#re.IGNORECASE\n",
      "#re.ASCII\n",
      "#re.LOCALE\n",
      "#re.IGNORECASE\n",
      "#re.ASCII\n",
      "#re.L\n",
      "#re.LOCALE\n",
      "#re.LOCALE\n",
      "#re.ASCII\n",
      "#re.LOCALE\n",
      "#re.M\n",
      "#re.MULTILINE\n",
      "#re.S\n",
      "#re.DOTALL\n",
      "#re.X\n",
      "#re.VERBOSE\n",
      "#re.search\n",
      "#match-objects\n",
      "#re.match\n",
      "#match-objects\n",
      "#re.MULTILINE\n",
      "#re.match\n",
      "#re.search\n",
      "#search-vs-match\n",
      "#re.fullmatch\n",
      "#match-objects\n",
      "#re.split\n",
      "#re.findall\n",
      "#re.finditer\n",
      "../glossary.html#term-iterator\n",
      "#match-objects\n",
      "#re.sub\n",
      "#match-objects\n",
      "#re-objects\n",
      "#re.subn\n",
      "#re.sub\n",
      "#re.escape\n",
      "#re.sub\n",
      "#re.subn\n",
      "#re.purge\n",
      "#re.error\n",
      "#re.error.msg\n",
      "#re.error.pattern\n",
      "#re.error.pos\n",
      "#re.error.lineno\n",
      "#re.error.colno\n",
      "#regular-expression-objects\n",
      "#re.Pattern.search\n",
      "#match-objects\n",
      "#re.Pattern.match\n",
      "#match-objects\n",
      "#re.Pattern.search\n",
      "#re.Pattern.search\n",
      "#search-vs-match\n",
      "#re.Pattern.fullmatch\n",
      "#match-objects\n",
      "#re.Pattern.search\n",
      "#re.Pattern.split\n",
      "#re.split\n",
      "#re.Pattern.findall\n",
      "#re.findall\n",
      "#re.search\n",
      "#re.Pattern.finditer\n",
      "#re.finditer\n",
      "#re.search\n",
      "#re.Pattern.sub\n",
      "#re.sub\n",
      "#re.Pattern.subn\n",
      "#re.subn\n",
      "#re.Pattern.flags\n",
      "#re.compile\n",
      "#re.Pattern.groups\n",
      "#re.Pattern.groupindex\n",
      "#re.Pattern.pattern\n",
      "copy.html#copy.copy\n",
      "copy.html#copy.deepcopy\n",
      "#match-objects\n",
      "#re.Pattern.match\n",
      "#re.Pattern.search\n",
      "#re.Match.expand\n",
      "#re.Pattern.sub\n",
      "#re.Match.group\n",
      "exceptions.html#IndexError\n",
      "exceptions.html#IndexError\n",
      "#re.Match.__getitem__\n",
      "#re.Match.groups\n",
      "#re.Match.groupdict\n",
      "#re.Match.start\n",
      "#re.Match.end\n",
      "exceptions.html#IndexError\n",
      "#re.Match.span\n",
      "#re.Match.pos\n",
      "#re.Pattern.search\n",
      "#re.Pattern.match\n",
      "#re-objects\n",
      "#re.Match.endpos\n",
      "#re.Pattern.search\n",
      "#re.Pattern.match\n",
      "#re-objects\n",
      "#re.Match.lastindex\n",
      "#re.Match.lastgroup\n",
      "#re.Match.re\n",
      "#re-objects\n",
      "#re.Pattern.match\n",
      "#re.Pattern.search\n",
      "#re.Match.string\n",
      "#re.Pattern.match\n",
      "#re.Pattern.search\n",
      "copy.html#copy.copy\n",
      "copy.html#copy.deepcopy\n",
      "#regular-expression-examples\n",
      "#checking-for-a-pair\n",
      "#re.Match.group\n",
      "#simulating-scanf\n",
      "#search-vs-match\n",
      "#re.match\n",
      "#re.search\n",
      "#re.search\n",
      "#re.MULTILINE\n",
      "#re.match\n",
      "#re.search\n",
      "#making-a-phonebook\n",
      "#re.split\n",
      "#re.split\n",
      "#text-munging\n",
      "#re.sub\n",
      "#re.sub\n",
      "#finding-all-adverbs\n",
      "#re.findall\n",
      "#re.search\n",
      "#re.findall\n",
      "#finding-all-adverbs-and-their-positions\n",
      "#re.finditer\n",
      "#match-objects\n",
      "#re.finditer\n",
      "#raw-string-notation\n",
      "#writing-a-tokenizer\n",
      "https://en.wikipedia.org/wiki/Lexical_analysis\n",
      "#id1\n",
      "../contents.html\n",
      "#\n",
      "#regular-expression-syntax\n",
      "#module-contents\n",
      "#regular-expression-objects\n",
      "#match-objects\n",
      "#regular-expression-examples\n",
      "#checking-for-a-pair\n",
      "#simulating-scanf\n",
      "#search-vs-match\n",
      "#making-a-phonebook\n",
      "#text-munging\n",
      "#finding-all-adverbs\n",
      "#finding-all-adverbs-and-their-positions\n",
      "#raw-string-notation\n",
      "#writing-a-tokenizer\n",
      "string.html\n",
      "difflib.html\n",
      "../bugs.html\n",
      "https://github.com/python/cpython/blob/3.8/Doc/library/re.rst\n",
      "../genindex.html\n",
      "../py-modindex.html\n",
      "difflib.html\n",
      "string.html\n",
      "https://www.python.org/\n",
      "../index.html\n",
      "index.html\n",
      "text.html\n",
      "../copyright.html\n",
      "https://www.python.org/psf/donations/\n",
      "https://docs.python.org/3/bugs.html\n",
      "https://www.sphinx-doc.org/\n"
     ]
    }
   ],
   "source": [
    "'''4. Write a Python program to extract all URL's from the\n",
    "specified link.'''\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = input(\"Enter the URL to scrape:\")\n",
    "r = requests.get(url)\n",
    "c = r.content\n",
    "soup = BeautifulSoup(c,\"html.parser\")\n",
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML Details\n",
      "b'<!doctype html>\\n<html>\\n<head>\\n    <title>Example Domain</title>\\n\\n    <meta charset=\"utf-8\" />\\n    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" />\\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\\n    <style type=\"text/css\">\\n    body {\\n        background-color: #f0f0f2;\\n        margin: 0;\\n        padding: 0;\\n        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\\n        \\n    }\\n    div {\\n        width: 600px;\\n        margin: 5em auto;\\n        padding: 2em;\\n        background-color: #fdfdff;\\n        border-radius: 0.5em;\\n        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\\n    }\\n    a:link, a:visited {\\n        color: #38488f;\\n        text-decoration: none;\\n    }\\n    @media (max-width: 700px) {\\n        div {\\n            margin: 0 auto;\\n            width: auto;\\n        }\\n    }\\n    </style>    \\n</head>\\n\\n<body>\\n<div>\\n    <h1>Example Domain</h1>\\n    <p>This domain is for use in illustrative examples in documents. You may use this\\n    domain in literature without prior coordination or asking for permission.</p>\\n    <p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\\n</div>\\n</body>\\n</html>\\n'\n"
     ]
    }
   ],
   "source": [
    "'''5. Write a Python program to test if a given page is\n",
    "found or not on the server.'''\n",
    "from urllib3.exceptions import HTTPError as BaseHTTPError\n",
    "import requests\n",
    "\n",
    "class RequestException(IOError):\n",
    "    \"\"\"This is the basis to raise errors\"\"\"\n",
    "    \n",
    "class HTTPError(RequestException):\n",
    "    \"\"\"An HTTP error occured.\"\"\"\n",
    "    \n",
    "class URLError(RequestException):\n",
    "    \"\"\"An HTTP error occured.\"\"\"\n",
    "    \n",
    "try:\n",
    "    html = requests.get(\"http://www.example.com/\")\n",
    "except HTTPError as e:\n",
    "    print(\"HTTP error\")\n",
    "except URLError as e:\n",
    "    print(\"Server not found!\")\n",
    "else:\n",
    "    print(\"HTML Details\")\n",
    "    print(html.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "This will show the most popular posts on social media\n",
      "------------------------------\n",
      "Post title: s\n",
      "Post title: u\n",
      "Post title: n\n",
      "Post title: t\n",
      "Post title:  \n",
      "Post title: a\n",
      "Post title: u\n",
      "Post title: t\n",
      "Post title:  \n",
      "Post title: f\n",
      "Post title: a\n",
      "Post title: c\n",
      "Post title: e\n",
      "Post title: r\n",
      "Post title: e\n",
      "Post title:  \n",
      "Post title: r\n",
      "Post title: e\n",
      "Post title: p\n",
      "Post title: e\n",
      "Post title: l\n",
      "Post title: l\n",
      "Post title: a\n",
      "Post title: t\n",
      "Post title:  \n",
      "Post title: p\n",
      "Post title: r\n",
      "Post title: o\n",
      "Post title: v\n",
      "Post title: i\n",
      "Post title: d\n",
      "Post title: e\n",
      "Post title: n\n",
      "Post title: t\n",
      "Post title:  \n",
      "Post title: o\n",
      "Post title: c\n",
      "Post title: c\n",
      "Post title: a\n",
      "Post title: e\n",
      "Post title: c\n",
      "Post title: a\n",
      "Post title: t\n",
      "Post title: i\n",
      "Post title:  \n",
      "Post title: e\n",
      "Post title: x\n",
      "Post title: c\n",
      "Post title: e\n",
      "Post title: p\n",
      "Post title: t\n",
      "Post title: u\n",
      "Post title: r\n",
      "Post title: i\n",
      "Post title:  \n",
      "Post title: o\n",
      "Post title: p\n",
      "Post title: t\n",
      "Post title: i\n",
      "Post title: o\n",
      "Post title:  \n",
      "Post title: r\n",
      "Post title: e\n",
      "Post title: p\n",
      "Post title: r\n",
      "Post title: e\n",
      "Post title: h\n",
      "Post title: e\n",
      "Post title: n\n",
      "Post title: d\n",
      "Post title: e\n",
      "Post title: r\n",
      "Post title: i\n",
      "Post title: t\n"
     ]
    }
   ],
   "source": [
    "'''6. Write a python program to fetch data using a test API\n",
    "and display it.'''\n",
    "import requests\n",
    "import json\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"This will show the most popular posts on social media\")\n",
    "print(\"-\"*30)\n",
    "data = requests.get(\"https://jsonplaceholder.typicode.com/posts\").json()\n",
    "for item in data[0]['title']:\n",
    "    print('Post title:',(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 days of visits broken down by browser for all sites:\n",
      "{'Chrome': 2533872868, 'Safari': 1842799197, 'Edge': 218081077, 'Internet Explorer': 191826159, 'Samsung Internet': 156852135, 'Firefox': 169453235, 'Safari (in-app)': 71782614, 'Android Webview': 59780833, 'Amazon Silk': 12201260, 'Opera': 13120471, 'Mercari_d': 5047347, 'UC Browser': 2649296, 'Opera Mini': 1955656, 'Mozilla Compatible Agent': 1434461, 'YaBrowser': 1180124, 'Coc Coc': 499802, 'Android Browser': 531686, 'ELB-HealthChecker': 563082, 'Amazon.com': 320767, 'Playstation 4': 203884, 'Puffin': 186634, 'Mozilla': 152901, 'BestBuy': 148978, 'SeaMonkey': 127671, 'ThousandEyes': 166448, 'UA-name:iOSDasherWebView; client-version: 2.54.2.1852.200411': 5905, 'BlackBerry': 83265, 'ulta-app-ios': 45834, 'Maxthon': 53798, 'Instagram 137.0.0.30.124': 5656, '(not set)': 22233, 'DoximityWebView': 90208, 'IRS - TRG19-01Loeim': 59617, 'United States Postal Service - TRG19-01Loeim': 59588, 'UA-name:iOSDasherWebView; client-version: 2.56.0.1918.200416': 22952, 'Iron': 31126, '[FBAN': 28139, 'success agent': 6807, 'MagentaNews': 30089, 'com.airfind.deltabrowser': 19172, 'Voxgov': 40116, 'BublupBot': 51524, 'Android Runtime': 39322, 'candroid': 18866, 'Netscape': 19297, 'Bluebeam Revu Browser - cef version: 57.0.0.0': 13219, 'Lore Document Collector 5a20b182ca368a24e6462a7ac6947051': 16554, 'Playstation 3': 12220, 'WordPress': 12026, 'mediacloud bot for open academic research': 2201, 'Papers': 35352, 'Nintendo Browser': 13478, 'ulta-app-android': 10359, 'Hexometer': 8246, 'Microsoft Office Word 2014': 12507, 'Carousel': 17082, 'MRCHROME': 8069, 'Windows NT 6.1': 5074, 'LeapPadUltimate': 8597, 'lua-resty-http': 6465, 'Android': 56380, 'Airbnb': 22766, 'ZhihuHybrid DefaultBrowser osee2unifiedRelease': 9375, 'Google-Test2': 6888, 'com.doximity.doximitydroid': 10337, 'Handshake iOS 2.2.3': 494, 'HRB-MOBILE-IOS-PHONE-TAXES-FACEID-9.7.0': 724, 'Playstation Vita Browser': 4418, 'Seznam': 6439, 'Emb': 10557, 'LeapPad3Explorer': 2501, 'Microsoft Office Excel 2014': 2913, 'NetFront': 3260, 'DDG-Android-3.1.0': 1944, 'HRB-MOBILE-IOS-PHONE-TAXES-TOUCHID-9.7.0': 498, 'com.seekingalpha.webwrapper': 591, 'Mobile IOS 9.0.1.1; Mozilla': 197, 'UA-name:iOSDasherWebView; client-version: 2.52.1.1438.200320': 176, 'Uzbl': 2661, '\"Mozilla': 4481, '+Simple Browser': 1760, 'Nokia Browser': 1655, 'Jetty': 2072, 'Mobile IOS 4.0.0.11; Mozilla': 329, 'LeapPadPlatinum': 2256, 'Mobile IOS 9.2.2.1; Mozilla': 2232, 'HRB-MOBILE-IOS-PHONE-TAXES-9.7.0': 167, 'Instagram 135.1.0.23.118': 173, 'WizenozeBot': 5650, 'chrome-Selenium': 1456, 'UA-name:iOSDasherWebView; client-version: 2.54.0.1719.200407': 126, 'Nexgate Ruby Client': 350, 'class s extends Function{constructor': 242, 'iPhone': 42711, 'LINER': 847, 'Nintendo 3DS Browser': 1217, 'mozilla': 574, 'Microsoft Office Word 2013': 794, 'Sogou web spider': 31, 'IE with Chrome Frame': 1023, 'Outlook-iOS': 649, 'Welltory': 714, 'pa11y': 4493, 'User_Agent': 81, 'LG-B470': 80, 'Lunascape': 1399, 'PolycomVVX Lync': 487, 'Bloomberg|iOS|13.3.1|5.19.0|11261182d1995ed58b9683d0c4fc798409d7beaa': 26, 'Coursera-Mobile 3.6.3': 41, 'HRB-MOBILE-IOS-PHONE-TAXES-TOUCHID-9.7.0-Mozilla': 43, 'Grammarly': 389, 'Tracker': 959, 'WordPress.com; https:': 258, 'osee2unifiedRelease': 459, 'Camino': 149, 'LiveSlides': 745, 'com.airfind.browser': 707, 'eQAfy_Website_Checker_Bot_https:': 692, 'Instagram 117.0.0.23.163': 72, 'Selenium': 265, 'android': 370, 'BreitbartApp': 140, \"'Mozilla\": 158, 'Instagram 138.0.0.32.117': 4656, 'Nokia6280': 870, 'AppleCoreMedia': 535, 'Mobile IOS 9.2.3.1; Mozilla': 266, 'Microsoft Office PowerPoint 2014': 168, 'colly - https:': 135, 'eRA Default Scan Policy': 11, 'Infowars Android': 286, 'Instagram 136.0.0.22.123': 1585, 'Microsoft Office Excel 2013': 141, 'Modus Communicate': 13, 'WebLoadPerf': 40, 'WordPress.com; http:': 13, 'Browser': 70, 'MauiBot': 12, 'Lore Document Collector 1bb1dbb613d0db2041e35f52fea672c7': 1182, 'Instagram 111.0.0.16.152': 670, 'aconway': 98, 'qbhttp': 97, 'YE': 281, 'TravelSmart': 313, 'UNL_SITEMASTER': 619, '192.168.25.97:9092': 1037, '[user-agent string]': 23, 'fuelbot': 68, 'InDesign': 60, 'neoload': 422, 'Instagram 132.1.0.21.129': 342, 'HRB-MOBILE-IOS-PHONE-TAXES-FACEID-9.7.0-Mozilla': 27, 'bitdiscovery': 31476, 'Handshake iOS 2.2.4': 2200, 'Instagram 112.0.0.17.121': 355, 'Instagram 138.0.0.28.117 Android': 31, 'NinjBot': 373, 'UA-name:iOSDasherWebView; client-version: 2.48.0.841.200219': 24, 'Instagram 137.0.0.34.123 Android': 18, 'Asana': 50, 'HCL Verse': 74, 'Bloomberg|iOS|13.3.1|5.20.0|b18ca9dd5d1b86c08b1bae0c6e3774aa77ef49aa': 66, 'https:': 72, 'Instagram 139.1.0.39.120': 4787, 'iPhone cakes-app': 27, 'Podcasts': 94, 'SpeedaNewsPicks': 144, 'JovemNerdApp': 612, 'AndroidDownloadManager': 445, 'UA-name:iOSDasherWebView; client-version: 2.58.0.2156.200425': 8804, 'iTunes': 271, 'AdobeAIR': 24, '172.18.13.10:9092': 4137, 'spyglassbot puppeteer': 114, 'YahooMobileSearch': 16, 'Rely Bot': 4043, 'Re-re Studio': 11, 'Screaming Frog SEO Spider': 1089, 'C4Test00': 15, 'PostmanRuntime': 180, 'news-collector': 14, 'http4s-blaze': 25, 'iPad': 380, 'Instagram 130.0.0.15.120': 3365, 'NerdWallet': 2698, 'media-bot': 71, 'Phantom.js bot': 692, 'DingTalkBot-LinkService': 1298, 'win10chrome76': 29, 'Lynx': 745, 'cnn-mobile-app': 34, 'node-fetch': 17, 'Instagram 140.0.0.25.126': 7753, 'HubSpot-Link-Resolver': 145, 'Instagram 101.0.0.12.119': 50, 'Instagram 139.0.0.33.121 Android': 43, 'broken-link-checker': 23, 'HRB-MOBILE-IOS-PHONE-TAXES-FACEID-9.8.0': 397, 'Ruby': 42, 'UCWEB': 15, 'WebsiteScreenshotCreator': 77, 'Instagram 126.0.0.13.120': 1204, 'Instagram 123.1.0.26.115': 1353, 'HRB-MOBILE-IOS-PHONE-TAXES-TOUCHID-9.8.0': 272, 'Venmo': 142, 'QuickTime': 26, 'UA-name:iOSDasherWebView; client-version: 2.60.0.2321.200501': 15940, 'http.rb': 89, 'Instagram 141.0.0.24.117': 5118, 'Instagram 140.0.0.30.126 Android': 42, 'Web App': 13, 'Coursera-Mobile 3.7.2': 11, 'Konqueror': 11, 'Microsoft.Data.Mashup': 11, 'Instagram 120.0.0.18.116': 772, 'Handshake iOS 2.2.5': 2078, 'Netsparker Agent': 86, 'Bloomberg|iOS|13.4.1|5.21.0|d5d05c178ce33be4d047d7aea8e2972c6f546e8d': 296, 'mofang_app': 12, 'Bloomberg|iOS|13.3.1|5.21.0|d5d05c178ce33be4d047d7aea8e2972c6f546e8d': 35, 'screeenly-bot 2.0': 72, 'Bloomberg|Android|10|5.21.0.2276979.d5d05c178|d5d05c178ce33be4d047d7aea8e2972c6f546e8d': 27, 'Docoloc': 22, 'Ephox': 11, 'Instagram 133.0.0.20.118': 53, 'newsmthApp': 13, 'CareDroidMedications': 23, 'HRB-MOBILE-IOS-PHONE-TAXES-9.8.0': 57, 'Booking.com Android App 22.3.1': 203, 'NewsSlide': 17, 'RiteAidMobile': 66, 'Coursera-Mobile 3.8.1': 193, 'UA-name:iOSDasherWebView; client-version: 2.62.0.2524.200508': 4049, 'Grailed': 34, 'Booking.App': 6481, '192.168.0.18': 19, 'NE3': 457, 'Instagram 142.0.0.22.109': 8401, 'HV Android Wrapper 0.1': 11, 'UA-name:iOSDasherWebView; client-version: 2.62.20.2760.200519': 10593, 'iMozilla': 22, 'Manticore 0.6.4': 52, 'freeweb2app': 27, 'Ocean': 21, 'Instagram 123.0.0.24.115': 230, 'GeenStijlAndroidApp': 11, 'Handshake iOS 2.3.0': 171, 'Watchville Android 1.5.2': 28, 'EPiServer LinkValidator': 14, 'Handshake iOS 2.3.1': 1492, 'WAS': 56, 'CastBox': 43, 'Instagram 143.0.0.20.122': 6212, 'blackboard ally - accessibility checker - http:': 34, 'HRB-MOBILE-IOS-PHONE-TAXES-FACEID-9.9.0': 995, 'lyft:android:9:6.33.3.1590119151': 1942, 'bma': 11, 'HRB-MOBILE-IOS-PHONE-TAXES-TOUCHID-9.9.0': 419, 'com.tophatter v2.103.7': 3853, 'Instagram 132.0.0.18.129': 1036, 'com.tophatter v3.0.0': 16475, 'com.tophatter v2.103.10': 768, 'lyft:android:10:6.33.3.1590119151': 832, 'sdfsdf': 404, 'lyft:android:8.1.0:6.33.3.1590119151': 381, 'com.tophatter v2.103.5': 278, 'lyft:android:8.0.0:6.33.3.1590119151': 141, 'com.tophatter v2.103.6': 187, 'com.lyft.android.driver:android:9:1002.62.3.1590119153': 57, 'lyft:android:7.0:6.33.3.1590119151': 69, '7Siters': 11, 'lyft:android:9:6.34.3.1590583701': 7948, 'Instagram 144.0.0.17.119': 4494, 'com.tophatter v2.101.1': 120, 'curb': 14, 'UA-name:iOSDasherWebView; client-version: 2.64.0.2878.200522': 3545, 'UA-name:iOSDasherWebView; client-version: 2.68.0.3047.200529': 20777, 'Microsoft Office PowerPoint 2013': 15, 'lyft:android:10:6.34.3.1590583701': 4883, 'Bloomberg|iOS|13.4.1|5.22.1|9711b09eb675cf0e3556545cbd97f1441e374cf6': 13, 'com.tophatter v2.100.0': 33, 'com.tophatter v4.0.1': 520, 'lyft:android:8.1.0:6.34.3.1590583701': 1759, 'lyft:android:8.0.0:6.34.3.1590583701': 919, 'PipelinePilot': 11, 'lyft:android:7.0:6.34.3.1590583701': 640, 'com.tophatter v4.0.3': 194, 'eXtyles WinINet 1.0': 17, 'Handshake iOS 2.3.2': 933, 'Citoid': 13, 'com.lyft.android.driver:android:10:1002.63.3.1590584246': 262, 'Opendi Screenshot Bot bot@opendi.com': 135110, 'com.lyft.android.driver:android:9:1002.63.3.1590584246': 300, 'lyft:android:7.1.1:6.34.3.1590583701': 224, 'lyft:android:9:6.35.3.1591222463': 8572, 'Instagram 145.0.0.17.119': 4206, 'lyft:android:6.0.1:6.34.3.1590583701': 166, 'com.lyft.android.driver:android:8.1.0:1002.63.3.1590584246': 54, 'lyft:android:10:6.35.3.1591222463': 6391, 'com.lyft.android.driver:android:8.0.0:1002.63.3.1590584246': 37, 'lyft:android:7.1.2:6.34.3.1590583701': 37, 'com.lyft.android.driver:android:9:1002.64.3.1591188541': 405, 'com.lyft.android.driver:android:7.0:1002.63.3.1590584246': 16, 'Bloomberg|iOS|13.4.1|5.23.0|d63663da08e3ad75571d9550d5fd685884e03392': 41, 'lyft:android:8.1.0:6.35.3.1591222463': 1858, 'com.tophatter v4.0.4': 225, 'WebKit': 12, 'com.lyft.android.driver:android:10:1002.62.3.1590119153': 12, 'com.lyft.android.driver:android:10:1002.64.3.1591188541': 399, 'lyft:android:8.0.0:6.35.3.1591222463': 1173, 'lyft:android:7.0:6.35.3.1591222463': 780, 'integrity': 26, 'lyft:android:6.0.1:6.35.3.1591222463': 176, 'UA-name:iOSDasherWebView; client-version: 2.70.1.3286.200610': 928, 'lyft:android:7.1.1:6.35.3.1591222463': 251, 'HRB-MOBILE-IOS-PHONE-TAXES-9.9.0': 109, 'UA-name:iOSDasherWebView; client-version: 2.70.2.3352.200612': 17386, 'Papers2 Thumbnail Helper': 41, 'Downcast': 13, 'liloiosapp-66.1.6 Mozilla': 37, 'com.lyft.android.driver:android:8.0.0:1002.64.3.1591188541': 11, 'lyft:android:7.1.2:6.35.3.1591222463': 22, 'Instagram 115.1.0.71.189': 1030, 'hogehoge': 126, 'Turnitin': 411, 'CourierAndroid 5.22.2 rv:434': 12, 'Rustbot': 302, 'Drupal': 13, 'newspaper': 81, 'lyft:android:9:6.36.3.1591794132': 6714, 'Instagram 145.0.0.32.119 Android': 27, 'lyft:android:10:6.36.3.1591794132': 5589, 'com.lyft.android.driver:android:8.1.0:1002.64.3.1591188541': 11, 'UA-name:iOSDasherWebView; client-version: 2.72.0.3380.200612': 22251, 'Handshake iOS 2.3.3': 1193, 'lyft:android:8.1.0:6.36.3.1591794132': 1314, 'Yewno-Open': 17, 'lyft:android:7.0:6.36.3.1591794132': 568, 'lyft:android:8.0.0:6.36.3.1591794132': 1004, 'SalesforceMobileSDK': 18, 'com.lyft.android.driver:android:9:1002.65.3.1591791298': 289, 'com.lyft.android.driver:android:10:1002.65.3.1591791298': 321, 'lyft:android:7.1.1:6.36.3.1591794132': 190, 'Kronos': 11, 'lyft:android:6.0.1:6.36.3.1591794132': 143, 'Instagram 146.0.0.21.122': 6625, 'Instagram 124.0.0.11.473': 39, 'Party City Mobile App; iOS iPhone; 13.5.1; 6.1.16.0 : 48;': 11, 'EcoSearch': 18, 'PollEvPresenter': 28, 'com.lyft.android.driver:android:8.0.0:1002.65.3.1591791298': 22, 'Accessibility scan - NLM staff': 15945, 'Coursera-Mobile 3.9.1': 73, 'lyft:android:9:6.37.3.1592395944': 5313, 'Instagram 139.0.0.34.120': 528, 'ds9 2.004.1': 14, 'lyft:android:10:6.37.3.1592395944': 4696, 'UA-name:iOSDasherWebView; client-version: 2.74.0.1': 21570, 'Handshake iOS 2.3.4': 1288, 'lyft:android:8.1.0:6.37.3.1592395944': 1028, 'lyft:android:8.0.0:6.37.3.1592395944': 775, 'SearchBlox': 42, 'lyft:android:7.0:6.37.3.1592395944': 470, 'com.lyft.android.driver:android:9:1002.66.3.1592396259': 240, 'com.lyft.android.driver:android:10:1002.66.3.1592396259': 253, 'Instagram 147.0.0.30.121': 5256, 'lyft:android:7.1.1:6.37.3.1592395944': 142, 'lyft:android:6.0.1:6.37.3.1592395944': 90, 'RANDOM': 47, 'TotalValidator': 31, 'Instagram 147.0.0.42.124 Android': 82, 'edbrowse': 187, 'ADPTablet': 205, 'lyft:android:9:6.38.3.1593001991': 7126, 'lyft:android:10:6.38.3.1593001991': 7065, 'UA-name:iOSDasherWebView; client-version: 2.76.0.3489.200626': 25875, 'lyft:android:8.1.0:6.38.3.1593001991': 1419, 'Bloomberg|iOS|13.5.1|5.24.0|b954812d89433af9535e6ab38f9141628dc46e27': 86, 'lyft:android:8.0.0:6.38.3.1593001991': 1183, 'VSE': 12, 'Java': 100, 'lyft:android:7.0:6.38.3.1593001991': 663, 'lyft:android:6.0.1:6.38.3.1593001991': 128, 'lyft:android:7.1.1:6.38.3.1593001991': 214, 'Instagram 148.0.0.26.120': 7750, 'com.lyft.android.driver:android:10:1002.67.3.1593002387': 266, 'com.lyft.android.driver:android:9:1002.67.3.1593002387': 215, 'GreenChoiceNow': 11, 'Connie': 23, 'axios': 20, 'Bloomberg|iOS|13.5.1|5.25.0|688ee79ba47d67452eae5db8faede63e8ee6801d': 401, 'Bloomberg|Android|10|5.25.0.2358368.688ee79ba|688ee79ba47d67452eae5db8faede63e8ee6801d': 68, 'UA-name:iOSDasherWebView; client-version: 2.78.0.3587.200702': 7170, 'QZONEJSSDK': 27, 'HRB-MOBILE-IOS-TABLET-TAXES-TOUCHID-9.9.0': 105, 'Android ExpediaBookings': 14, 'Autn-WKOOP': 56, 'com.tophatter v5.0.1': 85, 'PodcastAddict': 11, 'desktop': 1040, 'Turo': 14, 'CBC': 510}\n"
     ]
    }
   ],
   "source": [
    "'''7. Write a Python program to fetch visitor data on basis\n",
    "of browser of the past 90 days on data.gov'''\n",
    "import requests\n",
    "r = requests.get(\"https://analytics.usa.gov/data/live/browsers.json\")\n",
    "print(\"90 days of visits broken down by browser for all sites:\")\n",
    "print(r.json()['totals']['browser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a HTTPS URL: https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all\n",
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "'''8. Write a Python program to verify SSL certificates for\n",
    "HTTPS requests using requests module.'''\n",
    "import requests\n",
    "url=input(\"Enter a HTTPS URL: \")\n",
    "print(requests.get(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of magnitude 4.5+ earthquakes detected worldwide by the USGS: 455\n"
     ]
    }
   ],
   "source": [
    "'''9. Write a Python program to get the number\n",
    "earthquakes detected worldwide with magnitude\n",
    "greater than 4.5 by the USGS.'''\n",
    "import csv\n",
    "import requests\n",
    "csvurl = 'http://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/4.5_month.csv'\n",
    "sl = requests.get(csvurl).text.splitlines()\n",
    "rows = list(csv.DictReader(sl))\n",
    "print(\"The number of magnitude 4.5+ earthquakes detected worldwide by the USGS:\", len(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "The Prestige (2006)\n",
      "After a tragic accident, two stage magicians engage in a battle to create the ultimate illusion while sacrificing everything they have to outwit each other.\n",
      "--------------------------------------------\n",
      "Toy Story (1995)\n",
      "A cowboy doll is profoundly threatened and jealous when a new spaceman figure supplants him as top toy in a boy's room.\n",
      "--------------------------------------------\n",
      "12 Years a Slave (2013)\n",
      "In the antebellum United States,\n",
      "--------------------------------------------\n",
      "Fanny och Alexander (1982)\n",
      "Two young Swedish children experience the many comedies and tragedies of their family, the Ekdahls.\n",
      "--------------------------------------------\n",
      "L.A. Confidential (1997)\n",
      "As corruption grows in 1950s Los Angeles, three policemen - one strait-laced, one brutal, and one sleazy - investigate a series of murders with their own brand of justice.\n",
      "--------------------------------------------\n",
      "Lawrence of Arabia (1962)\n",
      "The story of\n",
      "--------------------------------------------\n",
      "Inside Out (2015)\n",
      "After young Riley is uprooted from her Midwest life and moved to San Francisco, her emotions - Joy, Fear, Anger, Disgust and Sadness - conflict on how best to navigate a new city, house, and school.\n",
      "--------------------------------------------\n",
      "Aladdin (1992)\n",
      "A kindhearted street urchin and a power-hungry Grand Vizier vie for a magic lamp that has the power to make their deepest wishes come true.\n",
      "--------------------------------------------\n",
      "Interstellar (2014)\n",
      "A team of explorers travel through a wormhole in space in an attempt to ensure humanity's survival.\n",
      "--------------------------------------------\n",
      "The Circus (1928)\n",
      "The Tramp finds work and the girl of his dreams at a circus.\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "'''10. Write a Python program to get movie name, year\n",
    "and a brief summary of 10 random movies.'''\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import random\n",
    "def get_imd_movies(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    movies = soup.find_all(\"td\", class_=\"titleColumn\")\n",
    "    random.shuffle(movies)\n",
    "    return movies\n",
    "def get_imd_summary(url):\n",
    "    movie_page = requests.get(url)\n",
    "    soup = BeautifulSoup(movie_page.text, 'html.parser')\n",
    "    return soup.find(\"div\", class_=\"summary_text\").contents[0].strip()\n",
    "def get_imd_movie_info(movie):\n",
    "    movie_title = movie.a.contents[0]\n",
    "    movie_year = movie.span.contents[0]\n",
    "    movie_url = 'http://www.imdb.com' + movie.a['href']\n",
    "    return movie_title, movie_year, movie_url\n",
    "def imd_movie_picker():\n",
    "    ctr=0\n",
    "    print(\"--------------------------------------------\")\n",
    "    for movie in get_imd_movies('http://www.imdb.com/chart/top'):\n",
    "        movie_title, movie_year, movie_url = get_imd_movie_info(movie)\n",
    "        movie_summary = get_imd_summary(movie_url)\n",
    "        print(movie_title, movie_year)\n",
    "        print(movie_summary)\n",
    "        print(\"--------------------------------------------\")\n",
    "        ctr=ctr+1\n",
    "        if (ctr==10):\n",
    "            break;\n",
    "if __name__ == '__main__':\n",
    "    imd_movie_picker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the city:Kathmandu\n",
      "\n",
      "City name not found...\n"
     ]
    }
   ],
   "source": [
    "'''11. Write a Python program to find the live weather\n",
    "report (temperature, wind speed) of a given city.'''\n",
    "import requests\n",
    "\n",
    "def weather_data(query):\n",
    "    res=requests.get('http://api.openweathermap.org/data/2.5/weather?'+query+'&APPID=b35975')\n",
    "    return res.json();\n",
    "def print_weather(result, city):\n",
    "    print(\"{}'s temperature: {}°C \".format(city,result['main']['temp']))\n",
    "    print(\"Wind speed: {} m/s\".format(result['wind']['speed']))\n",
    "    print(\"Description: {}\".format(result['weather'][0]['description']))\n",
    "    print(\"Weather: {}\".format(result['weather'][0]['main']))\n",
    "def main():\n",
    "    city=input('Enter the city:')\n",
    "    print()\n",
    "    try:\n",
    "        query='q='+city;\n",
    "        w_data=weather_data(query);\n",
    "        print_weather(w_data, city)\n",
    "        print()\n",
    "    except:\n",
    "        print('City name not found...')\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "English\n",
      "6 125 000+ articles\n",
      "\n",
      "\n",
      "æ¥æ¬èª\n",
      "1 218 000+ è¨äº\n",
      "\n",
      "\n",
      "EspaÃ±ol\n",
      "1 612 000+ artÃ­culos\n",
      "\n",
      "\n",
      "Deutsch\n",
      "2 458 000+ Artikel\n",
      "\n",
      "\n",
      "Ð ÑÑÑÐºÐ¸Ð¹\n",
      "1 644 000+ ÑÑÐ°ÑÐµÐ¹\n",
      "\n",
      "\n",
      "FranÃ§ais\n",
      "2 236 000+ articles\n",
      "\n",
      "\n",
      "Italiano\n",
      "1 623 000+ voci\n",
      "\n",
      "\n",
      "ä¸­æ\n",
      "1 130 000+ æ¢ç®\n",
      "\n",
      "\n",
      "PortuguÃªs\n",
      "1 039 000+ artigos\n",
      "\n",
      "\n",
      "Polski\n",
      "1 420 000+ haseÅ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''12. Write a Python program to list all language names\n",
    "and number of related articles in the order they appear\n",
    "in wikipedia.org.'''\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "html = requests.get('https://www.wikipedia.org/')\n",
    "bs = BeautifulSoup(html.text, \"html.parser\")\n",
    "nameList = bs.findAll('a', {'class' : 'link-box'})\n",
    "for name in nameList:\n",
    "    print(name.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input your account name on Twitter:@dinesh99981\n"
     ]
    },
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-a6fcf7236e18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input your account name on Twitter:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://twitter.com/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lxml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/test/machine_learning/lib/python3.6/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                     \u001b[0;34m\"Couldn't find a tree builder with the features you \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                     \u001b[0;34m\"requested: %s. Do you need to install a parser library?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m                     % \",\".join(features))\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;31m# At this point either we have a TreeBuilder instance in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "'''13. Write a Python program to count number of tweets\n",
    "by a given Twitter account.'''\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "handle = input('Input your account name on Twitter:')\n",
    "temp = requests.get('https://twitter.com/'+handle)\n",
    "bs = BeautifulSoup(temp.text,'lxml')\n",
    "\n",
    "try:\n",
    "    tweet_box = bs.find('li',{'class':'ProfileNav-item ProfileNav-item--tweets is-active'})\n",
    "    tweets= tweet_box.find('a').find('span',{'class':'ProfileNav-value'})\n",
    "    print(\"{} tweets {} number of tweets.\".format(handle,tweets.get('data-count')))\n",
    "except:\n",
    "    print('Account name not found...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Wikisearch Key: dinesh\n",
      "#mw-head\n",
      "#searchInput\n",
      "/wiki/Hindu_Mythology\n",
      "/wiki/South_Asia\n",
      "/wiki/Devanagari\n",
      "/wiki/Hinduism\n",
      "/wiki/Given_name\n",
      "/wiki/Sanskrit\n",
      "/wiki/Epithet\n",
      "#cite_note-1\n",
      "/wiki/Dinesh_(Kannada_actor)\n",
      "/wiki/Dinesh_Baboo\n",
      "/wiki/Dinesh_Chand\n",
      "/wiki/Dinesh_Chandra_Sen\n",
      "/wiki/Dinesh_D%27Souza\n",
      "/wiki/Christian_apologist\n",
      "/wiki/Dinesh_Gunawardena\n",
      "/wiki/Dinesh_Gupta\n",
      "/wiki/Dinesh_Karthik\n",
      "/wiki/Dinesh_Kumar_(choreographer)\n",
      "/wiki/Dinesh_Lamba\n",
      "/wiki/Dinesh_Mongia\n",
      "/wiki/Dinesh_Nandan_Sahay\n",
      "/wiki/Dinesh_Nayak\n",
      "/wiki/Dinesh_Patel\n",
      "/wiki/Dinesh_Prasad_Singh\n",
      "/wiki/Dinesh_Singh_(Uttar_Pradesh_politician)\n",
      "/wiki/Dinesh_Singh_(Punjab_politician)\n",
      "/w/index.php?title=Dinesh&action=edit&section=1\n",
      "#cite_ref-1\n",
      "/wiki/Monier_Monier-Williams\n",
      "http://www.sanskrit-lexicon.uni-koeln.de/scans/MWScan/2014/web/index.php\n",
      "/wiki/Given_name\n",
      "https://en.wikipedia.org/w/index.php?title=Special:WhatLinksHere/Dinesh&namespace=0\n",
      "https://en.wikipedia.org/w/index.php?title=Dinesh&oldid=964155047\n",
      "/wiki/Help:Category\n",
      "/wiki/Category:Given_names\n",
      "/wiki/Category:Indian_masculine_given_names\n",
      "/wiki/Category:Articles_containing_Sanskrit-language_text\n",
      "/wiki/Category:Articles_with_short_description\n",
      "/wiki/Category:All_set_index_articles\n",
      "/wiki/Special:MyTalk\n",
      "/wiki/Special:MyContributions\n",
      "/w/index.php?title=Special:CreateAccount&returnto=Dinesh\n",
      "/w/index.php?title=Special:UserLogin&returnto=Dinesh\n",
      "/wiki/Dinesh\n",
      "/wiki/Talk:Dinesh\n",
      "/wiki/Dinesh\n",
      "/w/index.php?title=Dinesh&action=edit\n",
      "/w/index.php?title=Dinesh&action=history\n",
      "/wiki/Main_Page\n",
      "/wiki/Main_Page\n",
      "/wiki/Wikipedia:Contents\n",
      "/wiki/Portal:Current_events\n",
      "/wiki/Special:Random\n",
      "/wiki/Wikipedia:About\n",
      "//en.wikipedia.org/wiki/Wikipedia:Contact_us\n",
      "https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en\n",
      "//shop.wikimedia.org\n",
      "/wiki/Help:Contents\n",
      "/wiki/Wikipedia:Community_portal\n",
      "/wiki/Special:RecentChanges\n",
      "/wiki/Wikipedia:File_Upload_Wizard\n",
      "/wiki/Special:WhatLinksHere/Dinesh\n",
      "/wiki/Special:RecentChangesLinked/Dinesh\n",
      "/wiki/Wikipedia:File_Upload_Wizard\n",
      "/wiki/Special:SpecialPages\n",
      "/w/index.php?title=Dinesh&oldid=964155047\n",
      "/w/index.php?title=Dinesh&action=info\n",
      "/w/index.php?title=Special:CiteThisPage&page=Dinesh&id=964155047&wpFormIdentifier=titleform\n",
      "https://www.wikidata.org/wiki/Special:EntityPage/Q1226314\n",
      "/w/index.php?title=Special:ElectronPdf&page=Dinesh&action=show-download-screen\n",
      "/w/index.php?title=Dinesh&printable=yes\n",
      "https://commons.wikimedia.org/wiki/Category:Dinesh_(given_name)\n",
      "https://de.wikipedia.org/wiki/Dinesh\n",
      "https://ta.wikipedia.org/wiki/%E0%AE%A4%E0%AE%BF%E0%AE%A9%E0%AF%87%E0%AE%B7%E0%AF%8D\n",
      "https://www.wikidata.org/wiki/Special:EntityPage/Q1226314#sitelinks-wikipedia\n",
      "//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License\n",
      "//creativecommons.org/licenses/by-sa/3.0/\n",
      "//foundation.wikimedia.org/wiki/Terms_of_Use\n",
      "//foundation.wikimedia.org/wiki/Privacy_policy\n",
      "//www.wikimediafoundation.org/\n",
      "https://foundation.wikimedia.org/wiki/Privacy_policy\n",
      "/wiki/Wikipedia:About\n",
      "/wiki/Wikipedia:General_disclaimer\n",
      "//en.wikipedia.org/wiki/Wikipedia:Contact_us\n",
      "https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute\n",
      "https://stats.wikimedia.org/#/en.wikipedia.org\n",
      "https://foundation.wikimedia.org/wiki/Cookie_statement\n",
      "//en.m.wikipedia.org/w/index.php?title=Dinesh&mobileaction=toggle_view_mobile\n",
      "https://wikimediafoundation.org/\n",
      "https://www.mediawiki.org/\n"
     ]
    }
   ],
   "source": [
    "'''14. Write a Python program to that retrieves an arbitrary\n",
    "Wikipedia page of the users choice and creates a list of\n",
    "links on that page.'''\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "page=input(\"Enter Wikisearch Key: \")\n",
    "html = requests.get(\"https://en.wikipedia.org/wiki/\"+page)\n",
    "bsObj = BeautifulSoup(html.text)\n",
    "for link in bsObj.findAll(\"a\"):\n",
    "    if 'href' in link.attrs:\n",
    "        print(link.attrs['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of security alerts issued by US-CERT in the current year:\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "'''15. Write a Python program to get the number of\n",
    "security alerts issued by US-CERT in the current\n",
    "year.'''\n",
    "import requests\n",
    "from lxml import html\n",
    "url = 'https://www.us-cert.gov/ncas/alerts'\n",
    "doc = html.fromstring(requests.get(url).text)\n",
    "print(\"The number of security alerts issued by US-CERT in the current year:\")\n",
    "print(len(doc.cssselect('.item-list li')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Customer_id       Loan_type Income\n",
      "0            1       Home Loan    30K\n",
      "1            2   Personal Loan    25K\n",
      "2            3  Education Loan     15\n",
      "3            4       Home Loan    40K\n",
      "4            5     Credit Loan    35K\n",
      "\n",
      "Transformed Numerical value:\n",
      "   Customer_id  Loan_type  Income\n",
      "0            0          2       2\n",
      "1            1          3       1\n",
      "2            2          1       0\n",
      "3            3          2       4\n",
      "4            4          0       3\n"
     ]
    }
   ],
   "source": [
    "'''20. Write a pyhton program to convert all the\n",
    "categorical data with ordinal values in the dataset into\n",
    "numerical form.'''\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data = {'Customer_id': pd.Series([1,2,3,4,5]),\n",
    "'Loan_type': pd.Series(['Home Loan',\n",
    "'Personal Loan',\n",
    "'Education Loan',\n",
    "'Home Loan',\n",
    "'Credit Loan']),\n",
    "'Income': pd.Series(['30K','25K','15','40K','35K'])}\n",
    "\n",
    "loan_info = pd.DataFrame(data)\n",
    "print(loan_info)\n",
    "print()\n",
    "labelencoder = LabelEncoder()\n",
    "loan_info_upd = loan_info.apply(labelencoder.fit_transform)\n",
    "print(\"Transformed Numerical value:\")\n",
    "print(loan_info_upd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot']\n",
      "\n",
      "Integer Encoding:\n",
      "[0 0 2 0 1 1 2 0 2 1]\n",
      "\n",
      "OneHotEncoding:\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n",
      "\n",
      "['cold']\n"
     ]
    }
   ],
   "source": [
    "'''21. Implement OneHotEncoder from sklearn to convert\n",
    "categorical data into numerically parsable data.'''\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "data = ['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot']\n",
    "values = list(data)\n",
    "print(values)\n",
    "print()\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(data)\n",
    "print(\"Integer Encoding:\")\n",
    "print(integer_encoded)\n",
    "print()\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded),1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(\"OneHotEncoding:\")\n",
    "print(onehot_encoded)\n",
    "print()\n",
    "# invert first example\n",
    "inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
    "print(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
